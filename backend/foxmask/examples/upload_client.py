# upload_client.py
import asyncio
import aiohttp
import json
import os
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass
from enum import Enum
import mimetypes
from tqdm import tqdm
import time


class UploadSourceType(str, Enum):
    SINGLE_FILE = "SINGLE_FILE"
    MULTIPLE_FILES = "MULTIPLE_FILES"
    SINGLE_DIRECTORY = "SINGLE_DIRECTORY"
    MULTIPLE_DIRECTORIES = "MULTIPLE_DIRECTORIES"


class UploadStrategy(str, Enum):
    SEQUENTIAL = "SEQUENTIAL"
    PARALLEL = "PARALLEL"


class FileType(str, Enum):
    IMAGE = "IMAGE"
    VIDEO = "VIDEO"
    AUDIO = "AUDIO"
    DOCUMENT = "DOCUMENT"
    PDF = "PDF"
    TEXT = "TEXT"
    OTHER = "OTHER"


@dataclass
class UploadConfig:
    """ä¸Šä¼ é…ç½®"""
    graphql_endpoint: str = "http://localhost:8888/graphql"
    rest_endpoint: str = "http://localhost:8888/api/upload/chunk"
    auth_token: str = "your-auth-token-here"
    default_chunk_size: int = 5 * 1024 * 1024  # 5MB
    max_parallel_uploads: int = 3
    timeout: int = 300


class UploadClient:
    """å®Œæ•´çš„æ–‡ä»¶ä¸Šä¼ å®¢æˆ·ç«¯"""
    
    def __init__(self, config: UploadConfig):
        self.config = config
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            headers={
                "Authorization": f"Bearer {self.config.auth_token}",
                "Content-Type": "application/json"
            },
            timeout=aiohttp.ClientTimeout(total=self.config.timeout)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def execute_graphql(self, query: str, variables: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ‰§è¡ŒGraphQLæŸ¥è¯¢"""
        payload = {"query": query}
        if variables:
            payload["variables"] = variables
        
        async with self.session.post(
            self.config.graphql_endpoint,
            json=payload
        ) as response:
            result = await response.json()
            if "errors" in result:
                raise Exception(f"GraphQLé”™è¯¯: {result['errors']}")
            return result["data"]
    
    async def upload_chunk_rest(self, chunk_data: Dict[str, Any]) -> Dict[str, Any]:
        """é€šè¿‡REST APIä¸Šä¼ åˆ†å—"""
        form_data = aiohttp.FormData()
        
        for key, value in chunk_data.items():
            if key == "chunk_data" and isinstance(value, bytes):
                form_data.add_field('chunk_data', value, filename='chunk.bin')
            else:
                form_data.add_field(key, str(value))
        
        async with self.session.post(
            self.config.rest_endpoint,
            data=form_data
        ) as response:
            return await response.json()
    
    def _determine_source_type(self, source_paths: List[str]) -> UploadSourceType:
        """ç¡®å®šæºç±»å‹"""
        if len(source_paths) == 1:
            if os.path.isfile(source_paths[0]):
                return UploadSourceType.SINGLE_FILE
            elif os.path.isdir(source_paths[0]):
                return UploadSourceType.SINGLE_DIRECTORY
        else:
            if all(os.path.isfile(path) for path in source_paths):
                return UploadSourceType.MULTIPLE_FILES
            elif all(os.path.isdir(path) for path in source_paths):
                return UploadSourceType.MULTIPLE_DIRECTORIES
            else:
                return UploadSourceType.MULTIPLE_FILES
        
        raise ValueError("æ— æ³•ç¡®å®šæºç±»å‹")
    
    def _validate_source_paths(self, source_paths: List[str], source_type: UploadSourceType):
        """éªŒè¯æºè·¯å¾„"""
        for path in source_paths:
            if not os.path.exists(path):
                raise FileNotFoundError(f"è·¯å¾„ä¸å­˜åœ¨: {path}")
            
            if source_type in [UploadSourceType.SINGLE_FILE, UploadSourceType.MULTIPLE_FILES]:
                if not os.path.isfile(path):
                    raise ValueError(f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {path}")
            else:
                if not os.path.isdir(path):
                    raise ValueError(f"è·¯å¾„ä¸æ˜¯ç›®å½•: {path}")
    
    def _detect_file_type(self, file_path: str) -> FileType:
        """æ£€æµ‹æ–‡ä»¶ç±»å‹"""
        extension = Path(file_path).suffix.lower()
        
        image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}
        video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.webm'}
        audio_extensions = {'.mp3', '.wav', '.flac', '.aac', '.ogg'}
        document_extensions = {'.doc', '.docx', '.txt', '.rtf', '.odt'}
        
        if extension in image_extensions:
            return FileType.IMAGE
        elif extension in video_extensions:
            return FileType.VIDEO
        elif extension in audio_extensions:
            return FileType.AUDIO
        elif extension in document_extensions:
            return FileType.DOCUMENT
        elif extension == '.pdf':
            return FileType.PDF
        elif extension == '.txt':
            return FileType.TEXT
        else:
            return FileType.OTHER
    
    async def _analyze_file(
        self, 
        file_path: str, 
        base_path: str,
        preserve_structure: bool,
        base_upload_path: str
    ) -> Dict[str, Any]:
        """åˆ†ææ–‡ä»¶ä¿¡æ¯"""
        try:
            file_stat = os.stat(file_path)
            file_size = file_stat.st_size
            
            # è®¡ç®—å­˜å‚¨è·¯å¾„
            if preserve_structure and os.path.isdir(base_path):
                relative_path = os.path.relpath(file_path, base_path)
                storage_path = os.path.join(base_upload_path, relative_path) if base_upload_path else relative_path
            else:
                storage_path = os.path.basename(file_path)
            
            # è®¡ç®—æ ¡éªŒå’Œï¼ˆåªå¯¹å°æ–‡ä»¶ï¼‰
            checksum_md5 = None
            checksum_sha256 = None
            
            if file_size < 10 * 1024 * 1024:  # 10MBä»¥ä¸‹è®¡ç®—æ ¡éªŒå’Œ
                try:
                    with open(file_path, 'rb') as f:
                        file_content = f.read()
                        checksum_md5 = hashlib.md5(file_content).hexdigest()
                        checksum_sha256 = hashlib.sha256(file_content).hexdigest()
                except Exception:
                    pass  # å¿½ç•¥æ ¡éªŒå’Œè®¡ç®—é”™è¯¯
            
            # è·å–æ–‡ä»¶ç±»å‹å’ŒMIMEç±»å‹
            file_type = self._detect_file_type(file_path)
            content_type, _ = mimetypes.guess_type(file_path)
            
            return {
                "filename": Path(file_path).name,
                "originalPath": file_path,
                "storagePath": storage_path,
                "fileSize": file_size,
                "fileType": file_type.value,
                "contentType": content_type or "application/octet-stream",
                "extension": Path(file_path).suffix.lower(),
                "checksumMd5": checksum_md5,
                "checksumSha256": checksum_sha256,
                "extractedMetadata": {
                    "createdTime": file_stat.st_ctime,
                    "modifiedTime": file_stat.st_mtime,
                },
                "relativePath": os.path.relpath(file_path, os.path.dirname(file_path)) if preserve_structure else None
            }
        except Exception as e:
            print(f"åˆ†ææ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None
    
    async def _discover_files(
        self,
        source_paths: List[str],
        source_type: UploadSourceType,
        preserve_structure: bool,
        base_upload_path: str,
        file_type_filters: List[FileType],
        max_file_size: int
    ) -> List[Dict[str, Any]]:
        """å‘ç°æ–‡ä»¶"""
        file_infos = []
        
        for source_path in source_paths:
            if source_type in [UploadSourceType.SINGLE_FILE, UploadSourceType.MULTIPLE_FILES]:
                # æ–‡ä»¶ä¸Šä¼ 
                file_info = await self._analyze_file(source_path, source_path, preserve_structure, base_upload_path)
                if file_info and self._filter_file(file_info, file_type_filters, max_file_size):
                    file_infos.append(file_info)
            else:
                # ç›®å½•ä¸Šä¼ 
                for root, dirs, files in os.walk(source_path):
                    for filename in files:
                        file_path = os.path.join(root, filename)
                        file_info = await self._analyze_file(file_path, source_path, preserve_structure, base_upload_path)
                        if file_info and self._filter_file(file_info, file_type_filters, max_file_size):
                            file_infos.append(file_info)
        
        return file_infos
    
    def _filter_file(self, file_info: Dict[str, Any], file_type_filters: List[FileType], max_file_size: int) -> bool:
        """æ–‡ä»¶è¿‡æ»¤"""
        if file_type_filters and file_info["fileType"] not in [f.value for f in file_type_filters]:
            return False
        
        if max_file_size and file_info["fileSize"] > max_file_size:
            print(f"æ–‡ä»¶è¶…è¿‡å¤§å°é™åˆ¶ {file_info['filename']}: {file_info['fileSize']} > {max_file_size}")
            return False
        
        return True
    
    async def upload(
        self,
        source_paths: Union[str, List[str]],
        title: str,
        description: str = None,
        upload_strategy: UploadStrategy = UploadStrategy.PARALLEL,
        chunk_size: int = None,
        preserve_structure: bool = True,
        base_upload_path: str = None,
        file_type_filters: List[FileType] = None,
        max_file_size: int = None,
        max_parallel_uploads: int = None
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€ä¸Šä¼ æ¥å£
        
        Args:
            source_paths: æºè·¯å¾„
            title: ä»»åŠ¡æ ‡é¢˜
            description: ä»»åŠ¡æè¿°
            upload_strategy: ä¸Šä¼ ç­–ç•¥
            chunk_size: åˆ†å—å¤§å°
            preserve_structure: æ˜¯å¦ä¿æŒç›®å½•ç»“æ„
            base_upload_path: åŸºç¡€ä¸Šä¼ è·¯å¾„
            file_type_filters: æ–‡ä»¶ç±»å‹è¿‡æ»¤å™¨
            max_file_size: æœ€å¤§æ–‡ä»¶å¤§å°
            max_parallel_uploads: æœ€å¤§å¹¶è¡Œä¸Šä¼ æ•°
        """
        start_time = time.time()
        
        try:
            # æ ‡å‡†åŒ–è¾“å…¥
            if isinstance(source_paths, str):
                source_paths = [source_paths]
            
            source_type = self._determine_source_type(source_paths)
            self._validate_source_paths(source_paths, source_type)
            
            # ä½¿ç”¨é»˜è®¤å€¼
            if chunk_size is None:
                chunk_size = self.config.default_chunk_size
            if max_parallel_uploads is None:
                max_parallel_uploads = self.config.max_parallel_uploads
            
            print(f"ğŸš€ å¼€å§‹ä¸Šä¼ : {title}")
            print(f"ğŸ“ ç±»å‹: {source_type.value}")
            print(f"ğŸ“ è·¯å¾„: {source_paths}")
            
            # é˜¶æ®µ1: åˆå§‹åŒ–ä»»åŠ¡
            init_result = await self._init_upload_task(
                source_paths=source_paths,
                source_type=source_type,
                title=title,
                description=description,
                upload_strategy=upload_strategy,
                chunk_size=chunk_size,
                preserve_structure=preserve_structure,
                base_upload_path=base_upload_path,
                file_type_filters=file_type_filters or [],
                max_file_size=max_file_size,
                max_parallel_uploads=max_parallel_uploads
            )
            
            print(f"âœ… åˆå§‹åŒ–å®Œæˆ: {init_result['total_files']} ä¸ªæ–‡ä»¶")
            
            # é˜¶æ®µ2: ä¸Šä¼ æ–‡ä»¶
            upload_results = await self._upload_files(
                init_result, 
                max_parallel_uploads
            )
            
            print(f"âœ… ä¸Šä¼ å®Œæˆ: æˆåŠŸ {upload_results['success']}, å¤±è´¥ {upload_results['failed']}")
            
            # é˜¶æ®µ3: å®Œæˆä»»åŠ¡
            complete_result = await self._complete_upload_task(
                init_result['task_id'], 
                force_complete=upload_results['failed'] > 0
            )
            
            duration = time.time() - start_time
            
            return {
                "success": upload_results['failed'] == 0,
                "message": f"ä¸Šä¼ å®Œæˆ: æˆåŠŸ {upload_results['success']}, å¤±è´¥ {upload_results['failed']}",
                "task_id": init_result['task_id'],
                "total_files": init_result['total_files'],
                "success_files": upload_results["success"],
                "failed_files": upload_results["failed"],
                "total_size": init_result['total_size'],
                "duration": f"{duration:.2f}ç§’",
                "complete_result": complete_result
            }
            
        except Exception as e:
            duration = time.time() - start_time
            return {
                "success": False,
                "message": f"ä¸Šä¼ å¤±è´¥: {str(e)}",
                "duration": f"{duration:.2f}ç§’"
            }
    
    async def _init_upload_task(
        self,
        source_paths: List[str],
        source_type: UploadSourceType,
        title: str,
        description: str,
        upload_strategy: UploadStrategy,
        chunk_size: int,
        preserve_structure: bool,
        base_upload_path: str,
        file_type_filters: List[FileType],
        max_file_size: int,
        max_parallel_uploads: int
    ) -> Dict[str, Any]:
        """åˆå§‹åŒ–ä¸Šä¼ ä»»åŠ¡"""
        # å‘ç°æ–‡ä»¶
        file_infos = await self._discover_files(
            source_paths, source_type, preserve_structure, 
            base_upload_path, file_type_filters, max_file_size
        )
        
        if not file_infos:
            raise Exception("æœªå‘ç°å¯ä¸Šä¼ çš„æ–‡ä»¶")
        
        total_size = sum(f["fileSize"] for f in file_infos)
        print(f"ğŸ“Š å‘ç° {len(file_infos)} ä¸ªæ–‡ä»¶, æ€»å¤§å°: {total_size / 1024 / 1024:.2f} MB")
        
        # GraphQLåˆå§‹åŒ–Mutation
        init_mutation = """
        mutation InitUploadTask($input: UploadTaskInitInput!) {
            initUploadTask(input: $input) {
                success
                message
                task_id
                data {
                    task {
                        id
                        chunkSize
                    }
                    totalFiles
                    totalSize
                    totalChunks
                }
            }
        }
        """
        
        init_variables = {
            "input": {
                "taskType": "BATCH_UPLOAD",
                "sourceType": source_type.value,
                "title": title,
                "desc": description or f"ä¸Šä¼ : {title}",
                "uploadStrategy": upload_strategy.value,
                "maxParallelUploads": max_parallel_uploads,
                "chunkSize": chunk_size,
                "preserveStructure": preserve_structure,
                "baseUploadPath": base_upload_path,
                "autoExtractMetadata": True,
                "fileTypeFilters": [f.value for f in file_type_filters],
                "maxFileSize": max_file_size,
                "files": file_infos
            }
        }
        
        print("ğŸ”„ åˆå§‹åŒ–ä¸Šä¼ ä»»åŠ¡...")
        result = await self.execute_graphql(init_mutation, init_variables)
        
        if not result["initUploadTask"]["success"]:
            raise Exception(f"åˆå§‹åŒ–å¤±è´¥: {result['initUploadTask']['message']}")
        
        return {
            "task_id": result["initUploadTask"]["task_id"],
            "total_files": result["initUploadTask"]["data"]["totalFiles"],
            "total_size": result["initUploadTask"]["data"]["totalSize"],
            "chunk_size": result["initUploadTask"]["data"]["task"]["chunkSize"],
            "file_infos": file_infos
        }
    
    async def _upload_files(
        self, 
        init_result: Dict[str, Any], 
        max_parallel_uploads: int
    ) -> Dict[str, Any]:
        """ä¸Šä¼ æ–‡ä»¶åˆ†å—"""
        print("ğŸ”„ å¼€å§‹ä¸Šä¼ æ–‡ä»¶...")
        
        success_count = 0
        failed_count = 0
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(max_parallel_uploads)
        
        # åˆ›å»ºä¸Šä¼ ä»»åŠ¡
        upload_tasks = []
        for file_info in init_result["file_infos"]:
            task = asyncio.create_task(
                self._upload_single_file(
                    init_result["task_id"],
                    file_info,
                    init_result["chunk_size"],
                    semaphore
                )
            )
            upload_tasks.append(task)
        
        # æ˜¾ç¤ºæ€»ä½“è¿›åº¦
        with tqdm(total=len(upload_tasks), desc="æ€»ä½“è¿›åº¦", unit="file") as pbar:
            results = await asyncio.gather(*upload_tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception) or not result:
                    failed_count += 1
                else:
                    success_count += 1
                pbar.update(1)
        
        return {"success": success_count, "failed": failed_count}
    
    async def _upload_single_file(
        self, 
        task_id: str, 
        file_info: Dict[str, Any], 
        chunk_size: int, 
        semaphore: asyncio.Semaphore
    ) -> bool:
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶"""
        async with semaphore:
            try:
                file_path = file_info["originalPath"]
                file_size = file_info["fileSize"]
                file_id = file_info["filename"]  # ç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨æ–‡ä»¶åä½œä¸ºfile_id
                
                total_chunks = (file_size + chunk_size - 1) // chunk_size
                
                # æ–‡ä»¶è¿›åº¦æ¡
                with tqdm(total=file_size, unit='B', unit_scale=True, 
                         desc=file_info["filename"][:20], leave=False) as pbar:
                    
                    with open(file_path, 'rb') as f:
                        for chunk_num in range(total_chunks):
                            chunk_data = f.read(chunk_size)
                            is_final_chunk = (chunk_num == total_chunks - 1)
                            
                            # è®¡ç®—æ ¡éªŒå’Œ
                            checksum_md5 = hashlib.md5(chunk_data).hexdigest()
                            checksum_sha256 = hashlib.sha256(chunk_data).hexdigest()
                            
                            # ä¸Šä¼ åˆ†å—
                            success = await self._upload_chunk(
                                task_id=task_id,
                                file_id=file_id,
                                chunk_number=chunk_num + 1,
                                chunk_data=chunk_data,
                                is_final_chunk=is_final_chunk,
                                checksum_md5=checksum_md5,
                                checksum_sha256=checksum_sha256
                            )
                            
                            if not success:
                                return False
                            
                            pbar.update(len(chunk_data))
                
                return True
                
            except Exception as e:
                print(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥ {file_info['filename']}: {e}")
                return False
    
    async def _upload_chunk(
        self,
        task_id: str,
        file_id: str,
        chunk_number: int,
        chunk_data: bytes,
        is_final_chunk: bool,
        checksum_md5: str,
        checksum_sha256: str
    ) -> bool:
        """ä¸Šä¼ å•ä¸ªåˆ†å—"""
        try:
            chunk_info = {
                "task_id": task_id,
                "file_id": file_id,
                "chunk_number": chunk_number,
                "chunk_data": chunk_data,
                "is_final_chunk": is_final_chunk,
                "checksum_md5": checksum_md5,
                "checksum_sha256": checksum_sha256
            }
            
            result = await self.upload_chunk_rest(chunk_info)
            return result.get("success", False)
            
        except Exception as e:
            print(f"åˆ†å—ä¸Šä¼ å¤±è´¥: {e}")
            return False
    
    async def _complete_upload_task(self, task_id: str, force_complete: bool = False) -> Dict[str, Any]:
        """å®Œæˆä¸Šä¼ ä»»åŠ¡"""
        print("ğŸ”„ å®Œæˆä»»åŠ¡...")
        
        complete_mutation = """
        mutation CompleteUploadTask($input: UploadTaskCompleteInput!) {
            completeUploadTask(input: $input) {
                success
                message
                completedFiles
                failedFiles
                totalFiles
            }
        }
        """
        
        complete_variables = {
            "input": {
                "taskId": task_id,
                "forceComplete": force_complete
            }
        }
        
        result = await self.execute_graphql(complete_mutation, complete_variables)
        return result["completeUploadTask"]
    
    async def get_task_progress(self, task_id: str) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡è¿›åº¦"""
        query = """
        query GetUploadProgress($id: String!) {
            uploadTask(id: $id) {
                taskStatus
                totalFiles
                completedFiles
                failedFiles
                totalSize
                uploadedSize
                progressPercentage
            }
        }
        """
        
        result = await self.execute_graphql(query, {"id": task_id})
        return result["uploadTask"]


async def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå„ç§ä¸Šä¼ åœºæ™¯"""
    
    # é…ç½®å®¢æˆ·ç«¯
    config = UploadConfig(
        graphql_endpoint="http://localhost:8000/graphql",
        rest_endpoint="http://localhost:8000/api/upload/chunk",
        auth_token="your-jwt-token-here",  # æ›¿æ¢ä¸ºå®é™…token
        max_parallel_uploads=2
    )
    
    async with UploadClient(config) as client:
        print("=" * 50)
        print("ğŸ“ æ–‡ä»¶ä¸Šä¼ å®¢æˆ·ç«¯æ¼”ç¤º")
        print("=" * 50)
        
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        test_dir = "test_upload_files"
        os.makedirs(test_dir, exist_ok=True)
        
        # åˆ›å»ºä¸€äº›æµ‹è¯•æ–‡ä»¶
        test_files = [
            ("small_file.txt", "è¿™æ˜¯ä¸€ä¸ªå°æ–‡æœ¬æ–‡ä»¶"),
            ("medium_file.txt", "è¿™æ˜¯ä¸€ä¸ªä¸­ç­‰å¤§å°çš„æ–‡ä»¶\n" * 1000),
        ]
        
        for filename, content in test_files:
            filepath = os.path.join(test_dir, filename)
            if not os.path.exists(filepath):
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(content)
                print(f"åˆ›å»ºæµ‹è¯•æ–‡ä»¶: {filepath}")
        
        # åœºæ™¯1: ä¸Šä¼ å•ä¸ªæ–‡ä»¶
        print("\n1. ğŸ“„ å•ä¸ªæ–‡ä»¶ä¸Šä¼ ")
        result1 = await client.upload(
            source_paths=os.path.join(test_dir, "small_file.txt"),
            title="æµ‹è¯•å•ä¸ªæ–‡ä»¶ä¸Šä¼ ",
            description="ä¸Šä¼ å•ä¸ªæ–‡æœ¬æ–‡ä»¶æµ‹è¯•"
        )
        print(f"ç»“æœ: {result1}")
        
        # åœºæ™¯2: ä¸Šä¼ å¤šä¸ªæ–‡ä»¶
        print("\n2. ğŸ“š å¤šä¸ªæ–‡ä»¶ä¸Šä¼ ")
        result2 = await client.upload(
            source_paths=[
                os.path.join(test_dir, "small_file.txt"),
                os.path.join(test_dir, "medium_file.txt")
            ],
            title="æµ‹è¯•å¤šæ–‡ä»¶ä¸Šä¼ ",
            description="ä¸Šä¼ å¤šä¸ªæ–‡æœ¬æ–‡ä»¶æµ‹è¯•",
            upload_strategy=UploadStrategy.PARALLEL
        )
        print(f"ç»“æœ: {result2}")
        
        # åœºæ™¯3: ä¸Šä¼ ç›®å½•
        print("\n3. ğŸ“ ç›®å½•ä¸Šä¼ ")
        result3 = await client.upload(
            source_paths=test_dir,
            title="æµ‹è¯•ç›®å½•ä¸Šä¼ ",
            description="ä¸Šä¼ æ•´ä¸ªç›®å½•æµ‹è¯•",
            preserve_structure=True,
            base_upload_path="test_uploads"
        )
        print(f"ç»“æœ: {result3}")
        
        # ç›‘æ§è¿›åº¦ç¤ºä¾‹
        if result3.get('task_id'):
            print("\n4. ğŸ“Š ç›‘æ§ä¸Šä¼ è¿›åº¦")
            for i in range(3):  # ç›‘æ§3æ¬¡
                progress = await client.get_task_progress(result3['task_id'])
                print(f"è¿›åº¦ {i+1}: {progress['progressPercentage']:.1f}%")
                await asyncio.sleep(1)


async def quick_start():
    """å¿«é€Ÿå¼€å§‹ç¤ºä¾‹"""
    config = UploadConfig(
        graphql_endpoint="http://localhost:8888/graphql",
        rest_endpoint="http://localhost:8888/api/upload/chunk",
        auth_token="your-token-here"
    )
    
    async with UploadClient(config) as client:
        # æœ€ç®€å•çš„ä¸Šä¼ ç¤ºä¾‹
        result = await client.upload(
            source_paths="path/to/your/file.txt",  # æ›¿æ¢ä¸ºå®é™…æ–‡ä»¶è·¯å¾„
            title="æˆ‘çš„ç¬¬ä¸€ä¸ªä¸Šä¼ ä»»åŠ¡"
        )
        print("ä¸Šä¼ ç»“æœ:", result)


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())
    
    # æˆ–è€…è¿è¡Œå¿«é€Ÿå¼€å§‹
    # asyncio.run(quick_start())